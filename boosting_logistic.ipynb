{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c521176d-0eaa-467b-ae73-6e90440213e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": {
        "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
        "text/plain": ""
       },
       "datasetInfos": [],
       "executionCount": null,
       "metadata": {
        "kernelSessionId": "2945677d-280e67b3ac7e1774908807b8"
       },
       "removedWidgets": [],
       "type": "mimeBundle"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ebe4bc8-da54-4f8d-a3dd-492a5e9ee103",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, IndexToString, OneHotEncoder, StandardScaler\n",
    "from pyspark.sql.functions import trim\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c34b67df-ca29-4148-a637-0ebf8252d4b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec75c59-b3f9-47a8-833a-9042fe4654e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loading Data from MongoDB as SparkDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b79d2e4-da90-48cb-b6d0-ae9d994a532d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "database = \"MongoDBAtlas\"\n",
    "user_name = \"aydinschwa\"\n",
    "password = \"pyJR1deIMz2KeY3i\"\n",
    "ip_address = \"chesscluster.ar0uw.mongodb.net\"\n",
    "collection_pos_eval = \"pos_evals\"\n",
    "collection_elo_eval = \"elo_eval\"\n",
    "connection_string_pos = f\"mongodb+srv://{user_name}:{password}@{ip_address}/{database}.{collection_pos_eval}\"\n",
    "connection_string_elo = f\"mongodb+srv://{user_name}:{password}@{ip_address}/{database}.{collection_elo_eval}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f14a5a80-5b19-4faa-b512-c5360edad16b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pos = spark.read.format(\"mongo\").option(\"uri\",connection_string_pos).load()\n",
    "df_eval = spark.read.format(\"mongo\").option(\"uri\",connection_string_elo).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "394174fc-eecf-4779-a02a-5b7be80027d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Processing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7191eda2-978a-4dc3-bbeb-83a75c1fa424",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creating a column 'elo_diff' which calculates the difference in ELO of the player with White pieces and black pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f34ba826-9d93-48d5-a756-d893738d0593",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_eval = df_eval.withColumn('elo_diff', df_eval['White Elo'] - df_eval['Black Elo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a0f381-ab11-413b-8e3e-63d95e8ffe46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creating a User Defined Function (UDF) to record the Expected scores. This metric is estimated using a formula that FIDE(Governing body of chess) uses to define the expected score of a game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd3680fa-b816-43c3-a800-5010dc9fa966",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_fide_expected_score(x):\n",
    "    return math.erfc(-x / ((2000.0/7) * math.sqrt(2))) / 2 ## Formula that FIDE(Governing body of chess) uses to calculate expected score of a game.\n",
    "\n",
    "xScore = udf(calculate_fide_expected_score, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27815f31-ad17-45b3-937f-1662fe0642c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_eval = df_eval.select('Black Elo', 'White Elo', 'Result', 'Time Class', 'Time Control','elo_diff',xScore(\"elo_diff\").alias(\"expected_score_fide\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff0741eb-0dd6-4b76-9a58-c4aacfc0aa65",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Transforming categorical variables through StringIndexing followed by OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc728e83-7387-48ce-af34-0995779d9107",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def indexStringColumns(df, cols):\n",
    "    # variable newdf will be updated several times\n",
    "    newdf = df\n",
    "    \n",
    "    for c in cols:\n",
    "        # For each given colum, fits StringIndexerModel.\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\").setHandleInvalid(\"keep\")\n",
    "        sm = si.fit(newdf)\n",
    "        \n",
    "        # Creates a DataFame by putting the transformed values in the new colum with suffix \"-num\" \n",
    "        # and then drops the original columns.\n",
    "        # and drop the \"-num\" suffix. \n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf\n",
    "\n",
    "def oneHotEncodeColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        # For each given colum, create OneHotEncoder. \n",
    "        # dropLast : Whether to drop the last category in the encoded vector (default: true)\n",
    "        ohe = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        ohe_model = ohe.fit(newdf)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-onehot\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-onehot\" suffix. \n",
    "        newdf = ohe_model.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56a7d07-959f-4d89-8446-876c72fd3703",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_cols = [\"Time Class\",\"Time Control\"]\n",
    "df_eval_sti = indexStringColumns(df_eval, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc573cde-d482-40ef-ba8b-b6204e09307b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_eval_ohe = oneHotEncodeColumns(df_eval_sti, categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c4db2a7-ea3a-4eb2-86ad-3435a9c792c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creating a UDF to convert string target variable to FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2d81edf-a1cd-4913-a074-ac70cfc829eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_res_to_binary(x):\n",
    "    if x == '1-0':\n",
    "        return 1\n",
    "    elif x == '0-1':\n",
    "        return 0\n",
    "    elif '5' in x:\n",
    "        return 2\n",
    "\n",
    "result_conv = udf(convert_res_to_binary, IntegerType())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e0d6920-82f7-4e2d-88c8-eba328b6d187",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_eval = df_eval_ohe.withColumn('result_int',result_conv('Result'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6254518-87ae-493b-be72-06f23aabc839",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Converting two string features(\"Black Elo\" and \"White Elo\") to Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2efd062-4d74-419b-8f85-d6235d6b675f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "int_conv = udf(convert_to_int, IntegerType())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42262fbc-1bb3-4f3a-b17f-61ff3494c4f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_eval = df_eval.select(int_conv('Black Elo').alias('Black Elo'), int_conv('White Elo').alias('White Elo'), 'Result', 'elo_diff', 'expected_score_fide', 'Time Class', 'Time Control', 'result_int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbf322a0-015a-4f0e-b557-6b9aca0c5201",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_eval = df_eval.where(df_eval.result_int != 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e845d53-9044-47a0-87c8-cc302f73fcf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------+--------+-------------------+-------------+--------------+----------+\n",
      "|Black Elo|White Elo|Result|elo_diff|expected_score_fide|   Time Class|  Time Control|result_int|\n",
      "+---------+---------+------+--------+-------------------+-------------+--------------+----------+\n",
      "|     2350|     2500|   1-0|   150.0|          0.7002084|(5,[2],[1.0])|(27,[4],[1.0])|         1|\n",
      "|     2646|     2331|   0-1|  -315.0|         0.13512218|(5,[2],[1.0])|(27,[4],[1.0])|         0|\n",
      "|     2287|     2317|   0-1|    30.0|          0.5418121|(5,[2],[1.0])|(27,[4],[1.0])|         0|\n",
      "|     2440|     2406|   1-0|   -34.0|         0.45263767|(5,[2],[1.0])|(27,[4],[1.0])|         1|\n",
      "|     2386|     2544|   1-0|   158.0|          0.7098683|(5,[2],[1.0])|(27,[4],[1.0])|         1|\n",
      "|     2778|     2746|   1-0|   -32.0|          0.4554117|(5,[0],[1.0])|(27,[0],[1.0])|         1|\n",
      "|     2646|     2736|   0-1|    90.0|          0.6236192|(5,[0],[1.0])|(27,[0],[1.0])|         0|\n",
      "|     2767|     2665|   1-0|  -102.0|          0.3605459|(5,[0],[1.0])|(27,[0],[1.0])|         1|\n",
      "|     2785|     2637|   0-1|  -148.0|         0.30222914|(5,[0],[1.0])|(27,[0],[1.0])|         0|\n",
      "|     2748|     2805|   1-0|    57.0|          0.5790642|(5,[0],[1.0])|(27,[0],[1.0])|         1|\n",
      "|     2771|     2878|   1-0|   107.0|          0.6459838|(5,[0],[1.0])|(27,[0],[1.0])|         1|\n",
      "|     2868|     2781|   1-0|   -87.0|          0.3803735|(5,[0],[1.0])|(27,[0],[1.0])|         1|\n",
      "|     2864|     2785|   0-1|   -79.0|         0.39108202|(5,[0],[1.0])|(27,[0],[1.0])|         0|\n",
      "|     2748|     2909|   1-0|   161.0|         0.71345276|(5,[0],[1.0])|(27,[8],[1.0])|         1|\n",
      "|     2908|     2761|   0-1|  -147.0|         0.30345124|(5,[0],[1.0])|(27,[8],[1.0])|         0|\n",
      "|     2625|     2417|   0-1|  -208.0|          0.2333068|(5,[2],[1.0])|(27,[4],[1.0])|         0|\n",
      "|     2529|     2453|   1-0|   -76.0|          0.3951196|(5,[2],[1.0])|(27,[4],[1.0])|         1|\n",
      "|     2514|     2770|   1-0|   256.0|         0.81487364|(5,[0],[1.0])|(27,[2],[1.0])|         1|\n",
      "|     2772|     2431|   0-1|  -341.0|         0.11633681|(5,[0],[1.0])|(27,[2],[1.0])|         0|\n",
      "|     2767|     2926|   1-0|   159.0|          0.7110655|(5,[0],[1.0])|(27,[2],[1.0])|         1|\n",
      "+---------+---------+------+--------+-------------------+-------------+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_eval.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d50229-8bb0-4f24-8281-8b85b740100b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create a dataframe with features and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50c7def-0ef6-41e8-a4b0-83243fd6df47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [
        {
         "name": "va_df",
         "schema": {
          "fields": [
           {
            "metadata": {
             "ml_attr": {
              "attrs": {
               "numeric": [
                {
                 "idx": 0,
                 "name": "Black Elo"
                },
                {
                 "idx": 1,
                 "name": "White Elo"
                }
               ]
              },
              "num_attrs": 2
             }
            },
            "name": "features",
            "nullable": true,
            "type": {
             "class": "org.apache.spark.ml.linalg.VectorUDT",
             "pyClass": "pyspark.ml.linalg.VectorUDT",
             "sqlType": {
              "fields": [
               {
                "metadata": {},
                "name": "type",
                "nullable": false,
                "type": "byte"
               },
               {
                "metadata": {},
                "name": "size",
                "nullable": true,
                "type": "integer"
               },
               {
                "metadata": {},
                "name": "indices",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "integer",
                 "type": "array"
                }
               },
               {
                "metadata": {},
                "name": "values",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "double",
                 "type": "array"
                }
               }
              ],
              "type": "struct"
             },
             "type": "udt"
            }
           },
           {
            "metadata": {},
            "name": "label",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# va = VectorAssembler(outputCol=\"features\", inputCols=[\"Black Elo\", \"White Elo\", \"elo_diff\", \"Time Class\", \"Time Control\"])\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=[\"Black Elo\", \"White Elo\"])\n",
    "va_df = va.transform(df_eval).select(\"features\", \"result_int\").withColumnRenamed(\"result_int\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f79528c-d6f1-4407-bf4f-6bbe98ab73a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [
        {
         "name": "va_df",
         "schema": {
          "fields": [
           {
            "metadata": {
             "ml_attr": {
              "attrs": {
               "numeric": [
                {
                 "idx": 0,
                 "name": "elo_diff"
                }
               ]
              },
              "num_attrs": 1
             }
            },
            "name": "features",
            "nullable": true,
            "type": {
             "class": "org.apache.spark.ml.linalg.VectorUDT",
             "pyClass": "pyspark.ml.linalg.VectorUDT",
             "sqlType": {
              "fields": [
               {
                "metadata": {},
                "name": "type",
                "nullable": false,
                "type": "byte"
               },
               {
                "metadata": {},
                "name": "size",
                "nullable": true,
                "type": "integer"
               },
               {
                "metadata": {},
                "name": "indices",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "integer",
                 "type": "array"
                }
               },
               {
                "metadata": {},
                "name": "values",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "double",
                 "type": "array"
                }
               }
              ],
              "type": "struct"
             },
             "type": "udt"
            }
           },
           {
            "metadata": {},
            "name": "label",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# experiment with just white Elo and black Elo as predictors\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=[\"elo_diff\"])\n",
    "va_df = va.transform(df_eval).select(\"features\", \"result_int\").withColumnRenamed(\"result_int\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63fd4a61-1517-4025-bc86-7698933ad2ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+--------+-----+\n",
       "features|label|\n",
       "+--------+-----+\n",
       " [150.0]|    1|\n",
       "[-315.0]|    0|\n",
       "  [30.0]|    0|\n",
       " [-34.0]|    1|\n",
       " [158.0]|    1|\n",
       " [-32.0]|    1|\n",
       "  [90.0]|    0|\n",
       "[-102.0]|    1|\n",
       "[-148.0]|    0|\n",
       "  [57.0]|    1|\n",
       " [107.0]|    1|\n",
       " [-87.0]|    1|\n",
       " [-79.0]|    0|\n",
       " [161.0]|    1|\n",
       "[-147.0]|    0|\n",
       "[-208.0]|    0|\n",
       " [-76.0]|    1|\n",
       " [256.0]|    1|\n",
       "[-341.0]|    0|\n",
       " [159.0]|    1|\n",
       "+--------+-----+\n",
       "only showing top 20 rows\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+--------+-----+\n|features|label|\n+--------+-----+\n| [150.0]|    1|\n|[-315.0]|    0|\n|  [30.0]|    0|\n| [-34.0]|    1|\n| [158.0]|    1|\n| [-32.0]|    1|\n|  [90.0]|    0|\n|[-102.0]|    1|\n|[-148.0]|    0|\n|  [57.0]|    1|\n| [107.0]|    1|\n| [-87.0]|    1|\n| [-79.0]|    0|\n| [161.0]|    1|\n|[-147.0]|    0|\n|[-208.0]|    0|\n| [-76.0]|    1|\n| [256.0]|    1|\n|[-341.0]|    0|\n| [159.0]|    1|\n+--------+-----+\nonly showing top 20 rows\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "va_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d7a3033-d4e5-4bcf-8990-3c80e14c0149",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Scale the data for regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aefff535-f049-4447-8d4f-499c66e4de14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [
        {
         "name": "scaled_df",
         "schema": {
          "fields": [
           {
            "metadata": {
             "ml_attr": {
              "num_attrs": 1
             }
            },
            "name": "features",
            "nullable": true,
            "type": {
             "class": "org.apache.spark.ml.linalg.VectorUDT",
             "pyClass": "pyspark.ml.linalg.VectorUDT",
             "sqlType": {
              "fields": [
               {
                "metadata": {},
                "name": "type",
                "nullable": false,
                "type": "byte"
               },
               {
                "metadata": {},
                "name": "size",
                "nullable": true,
                "type": "integer"
               },
               {
                "metadata": {},
                "name": "indices",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "integer",
                 "type": "array"
                }
               },
               {
                "metadata": {},
                "name": "values",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "double",
                 "type": "array"
                }
               }
              ],
              "type": "struct"
             },
             "type": "udt"
            }
           },
           {
            "metadata": {},
            "name": "label",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate the StandardScaler object\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withMean=True, withStd=True)\n",
    "\n",
    "# Fit the scaler to the data\n",
    "scaler_model = scaler.fit(va_df)\n",
    "\n",
    "# Transform the data using the scaler\n",
    "scaled_df = scaler_model.transform(va_df)\n",
    "\n",
    "# Select the scaled feature vector and 'result_int' columns\n",
    "scaled_df = scaled_df.select('scaled_features', 'label').withColumnRenamed(\"scaled_features\", \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc445250-a17b-4a30-b505-86b2b591ff27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Modeling Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "983be9df-0ec3-4b01-8fe4-a19931b4e135",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [
        {
         "name": "train",
         "schema": {
          "fields": [
           {
            "metadata": {
             "ml_attr": {
              "num_attrs": 1
             }
            },
            "name": "features",
            "nullable": true,
            "type": {
             "class": "org.apache.spark.ml.linalg.VectorUDT",
             "pyClass": "pyspark.ml.linalg.VectorUDT",
             "sqlType": {
              "fields": [
               {
                "metadata": {},
                "name": "type",
                "nullable": false,
                "type": "byte"
               },
               {
                "metadata": {},
                "name": "size",
                "nullable": true,
                "type": "integer"
               },
               {
                "metadata": {},
                "name": "indices",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "integer",
                 "type": "array"
                }
               },
               {
                "metadata": {},
                "name": "values",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "double",
                 "type": "array"
                }
               }
              ],
              "type": "struct"
             },
             "type": "udt"
            }
           },
           {
            "metadata": {},
            "name": "label",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        },
        {
         "name": "test",
         "schema": {
          "fields": [
           {
            "metadata": {
             "ml_attr": {
              "num_attrs": 1
             }
            },
            "name": "features",
            "nullable": true,
            "type": {
             "class": "org.apache.spark.ml.linalg.VectorUDT",
             "pyClass": "pyspark.ml.linalg.VectorUDT",
             "sqlType": {
              "fields": [
               {
                "metadata": {},
                "name": "type",
                "nullable": false,
                "type": "byte"
               },
               {
                "metadata": {},
                "name": "size",
                "nullable": true,
                "type": "integer"
               },
               {
                "metadata": {},
                "name": "indices",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "integer",
                 "type": "array"
                }
               },
               {
                "metadata": {},
                "name": "values",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "double",
                 "type": "array"
                }
               }
              ],
              "type": "struct"
             },
             "type": "udt"
            }
           },
           {
            "metadata": {},
            "name": "label",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create train/test split\n",
    "splits = scaled_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "train = splits[0].cache()\n",
    "test = splits[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478ec89c-3551-4157-8437-275644c9b08d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">AUC: 0.7458685600153211\n",
       "Accuracy: 0.6951983298538622\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">AUC: 0.7458685600153211\nAccuracy: 0.6951983298538622\n</div>",
       "datasetInfos": [
        {
         "name": "predictions",
         "schema": {
          "fields": [
           {
            "metadata": {
             "ml_attr": {
              "num_attrs": 1
             }
            },
            "name": "features",
            "nullable": true,
            "type": {
             "class": "org.apache.spark.ml.linalg.VectorUDT",
             "pyClass": "pyspark.ml.linalg.VectorUDT",
             "sqlType": {
              "fields": [
               {
                "metadata": {},
                "name": "type",
                "nullable": false,
                "type": "byte"
               },
               {
                "metadata": {},
                "name": "size",
                "nullable": true,
                "type": "integer"
               },
               {
                "metadata": {},
                "name": "indices",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "integer",
                 "type": "array"
                }
               },
               {
                "metadata": {},
                "name": "values",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "double",
                 "type": "array"
                }
               }
              ],
              "type": "struct"
             },
             "type": "udt"
            }
           },
           {
            "metadata": {},
            "name": "label",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {
             "ml_attr": {
              "num_attrs": 2
             }
            },
            "name": "rawPrediction",
            "nullable": true,
            "type": {
             "class": "org.apache.spark.ml.linalg.VectorUDT",
             "pyClass": "pyspark.ml.linalg.VectorUDT",
             "sqlType": {
              "fields": [
               {
                "metadata": {},
                "name": "type",
                "nullable": false,
                "type": "byte"
               },
               {
                "metadata": {},
                "name": "size",
                "nullable": true,
                "type": "integer"
               },
               {
                "metadata": {},
                "name": "indices",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "integer",
                 "type": "array"
                }
               },
               {
                "metadata": {},
                "name": "values",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "double",
                 "type": "array"
                }
               }
              ],
              "type": "struct"
             },
             "type": "udt"
            }
           },
           {
            "metadata": {
             "ml_attr": {
              "num_attrs": 2
             }
            },
            "name": "probability",
            "nullable": true,
            "type": {
             "class": "org.apache.spark.ml.linalg.VectorUDT",
             "pyClass": "pyspark.ml.linalg.VectorUDT",
             "sqlType": {
              "fields": [
               {
                "metadata": {},
                "name": "type",
                "nullable": false,
                "type": "byte"
               },
               {
                "metadata": {},
                "name": "size",
                "nullable": true,
                "type": "integer"
               },
               {
                "metadata": {},
                "name": "indices",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "integer",
                 "type": "array"
                }
               },
               {
                "metadata": {},
                "name": "values",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "double",
                 "type": "array"
                }
               }
              ],
              "type": "struct"
             },
             "type": "udt"
            }
           },
           {
            "metadata": {
             "ml_attr": {
              "num_vals": 2,
              "type": "nominal"
             }
            },
            "name": "prediction",
            "nullable": false,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# basic logistic regression\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "model = lr.fit(train)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"AUC:\", accuracy)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\\\n",
    "                .setLabelCol(\"label\")\\\n",
    "                .setPredictionCol(\"prediction\")\n",
    "evaluator.setMetricName(\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b4ddc7b-4ef0-432b-8572-d1d42fea419e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Hyperparameter Tuning Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bae4342-a62f-4a07-b603-33a612e0abcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:800: UserWarning: Cannot find mlflow module. To enable MLflow logging, install mlflow from PyPI.\n",
       "  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\n",
       "AUC score is 0.7820164609053514\n",
       "AUC score is 0.7458685600153211\n",
       "PR score is 0.7957062589554604\n",
       "PR score is 0.7628482508696373\n",
       "PR score is 0.711148034178386\n",
       "PR score is 0.6864570110830643\n",
       "Accuracy is 0.7136231884057971\n",
       "Accuracy is 0.6917188587334725\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:800: UserWarning: Cannot find mlflow module. To enable MLflow logging, install mlflow from PyPI.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\nAUC score is 0.7820164609053514\nAUC score is 0.7458685600153211\nPR score is 0.7957062589554604\nPR score is 0.7628482508696373\nPR score is 0.711148034178386\nPR score is 0.6864570110830643\nAccuracy is 0.7136231884057971\nAccuracy is 0.6917188587334725\n</div>",
       "datasetInfos": [
        {
         "name": "predict_train",
         "schema": {
          "fields": [
           {
            "metadata": {
             "ml_attr": {
              "num_attrs": 1
             }
            },
            "name": "features",
            "nullable": true,
            "type": {
             "class": "org.apache.spark.ml.linalg.VectorUDT",
             "pyClass": "pyspark.ml.linalg.VectorUDT",
             "sqlType": {
              "fields": [
               {
                "metadata": {},
                "name": "type",
                "nullable": false,
                "type": "byte"
               },
               {
                "metadata": {},
                "name": "size",
                "nullable": true,
                "type": "integer"
               },
               {
                "metadata": {},
                "name": "indices",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "integer",
                 "type": "array"
                }
               },
               {
                "metadata": {},
                "name": "values",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "double",
                 "type": "array"
                }
               }
              ],
              "type": "struct"
             },
             "type": "udt"
            }
           },
           {
            "metadata": {},
            "name": "label",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {
             "ml_attr": {
              "num_attrs": 2
             }
            },
            "name": "rawPrediction",
            "nullable": true,
            "type": {
             "class": "org.apache.spark.ml.linalg.VectorUDT",
             "pyClass": "pyspark.ml.linalg.VectorUDT",
             "sqlType": {
              "fields": [
               {
                "metadata": {},
                "name": "type",
                "nullable": false,
                "type": "byte"
               },
               {
                "metadata": {},
                "name": "size",
                "nullable": true,
                "type": "integer"
               },
               {
                "metadata": {},
                "name": "indices",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "integer",
                 "type": "array"
                }
               },
               {
                "metadata": {},
                "name": "values",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "double",
                 "type": "array"
                }
               }
              ],
              "type": "struct"
             },
             "type": "udt"
            }
           },
           {
            "metadata": {
             "ml_attr": {
              "num_attrs": 2
             }
            },
            "name": "probability",
            "nullable": true,
            "type": {
             "class": "org.apache.spark.ml.linalg.VectorUDT",
             "pyClass": "pyspark.ml.linalg.VectorUDT",
             "sqlType": {
              "fields": [
               {
                "metadata": {},
                "name": "type",
                "nullable": false,
                "type": "byte"
               },
               {
                "metadata": {},
                "name": "size",
                "nullable": true,
                "type": "integer"
               },
               {
                "metadata": {},
                "name": "indices",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "integer",
                 "type": "array"
                }
               },
               {
                "metadata": {},
                "name": "values",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "double",
                 "type": "array"
                }
               }
              ],
              "type": "struct"
             },
             "type": "udt"
            }
           },
           {
            "metadata": {
             "ml_attr": {
              "num_vals": 2,
              "type": "nominal"
             }
            },
            "name": "prediction",
            "nullable": false,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        },
        {
         "name": "predict_test",
         "schema": {
          "fields": [
           {
            "metadata": {
             "ml_attr": {
              "num_attrs": 1
             }
            },
            "name": "features",
            "nullable": true,
            "type": {
             "class": "org.apache.spark.ml.linalg.VectorUDT",
             "pyClass": "pyspark.ml.linalg.VectorUDT",
             "sqlType": {
              "fields": [
               {
                "metadata": {},
                "name": "type",
                "nullable": false,
                "type": "byte"
               },
               {
                "metadata": {},
                "name": "size",
                "nullable": true,
                "type": "integer"
               },
               {
                "metadata": {},
                "name": "indices",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "integer",
                 "type": "array"
                }
               },
               {
                "metadata": {},
                "name": "values",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "double",
                 "type": "array"
                }
               }
              ],
              "type": "struct"
             },
             "type": "udt"
            }
           },
           {
            "metadata": {},
            "name": "label",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {
             "ml_attr": {
              "num_attrs": 2
             }
            },
            "name": "rawPrediction",
            "nullable": true,
            "type": {
             "class": "org.apache.spark.ml.linalg.VectorUDT",
             "pyClass": "pyspark.ml.linalg.VectorUDT",
             "sqlType": {
              "fields": [
               {
                "metadata": {},
                "name": "type",
                "nullable": false,
                "type": "byte"
               },
               {
                "metadata": {},
                "name": "size",
                "nullable": true,
                "type": "integer"
               },
               {
                "metadata": {},
                "name": "indices",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "integer",
                 "type": "array"
                }
               },
               {
                "metadata": {},
                "name": "values",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "double",
                 "type": "array"
                }
               }
              ],
              "type": "struct"
             },
             "type": "udt"
            }
           },
           {
            "metadata": {
             "ml_attr": {
              "num_attrs": 2
             }
            },
            "name": "probability",
            "nullable": true,
            "type": {
             "class": "org.apache.spark.ml.linalg.VectorUDT",
             "pyClass": "pyspark.ml.linalg.VectorUDT",
             "sqlType": {
              "fields": [
               {
                "metadata": {},
                "name": "type",
                "nullable": false,
                "type": "byte"
               },
               {
                "metadata": {},
                "name": "size",
                "nullable": true,
                "type": "integer"
               },
               {
                "metadata": {},
                "name": "indices",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "integer",
                 "type": "array"
                }
               },
               {
                "metadata": {},
                "name": "values",
                "nullable": true,
                "type": {
                 "containsNull": false,
                 "elementType": "double",
                 "type": "array"
                }
               }
              ],
              "type": "struct"
             },
             "type": "udt"
            }
           },
           {
            "metadata": {
             "ml_attr": {
              "num_vals": 2,
              "type": "nominal"
             }
            },
            "name": "prediction",
            "nullable": false,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ParamGrid tries LASSO, ridge, and ElasticNet so no need to try them separately\n",
    "lr = LogisticRegression()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.elasticNetParam,[0.0, 0.5, 1.0])\\\n",
    "    .addGrid(lr.regParam,[0.01, 0.5, 2.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validation\n",
    "model_cv = cv.fit(train)\n",
    "\n",
    "predict_train = model_cv.transform(train)\n",
    "predict_test = model_cv.transform(test)\n",
    "\n",
    "# auc\n",
    "auc_train = evaluator.evaluate(predict_train)\n",
    "auc_test = evaluator.evaluate(predict_test)\n",
    "print(f\"AUC score is {auc_train}\")\n",
    "print(f\"AUC score is {auc_test}\")\n",
    "\n",
    "# pr\n",
    "evaluator.setMetricName(\"areaUnderPR\")\n",
    "pr_train = evaluator.evaluate(predict_train)\n",
    "pr_test = evaluator.evaluate(predict_test)\n",
    "print(f\"PR score is {pr_train}\")\n",
    "print(f\"PR score is {pr_test}\")\n",
    "\n",
    "# f1\n",
    "evaluator = MulticlassClassificationEvaluator()\\\n",
    "                .setLabelCol(\"label\")\\\n",
    "                .setPredictionCol(\"prediction\")\n",
    "evaluator.setMetricName(\"f1\") \n",
    "f1_train = evaluator.evaluate(predict_train)\n",
    "f1_test = evaluator.evaluate(predict_test)\n",
    "print(f\"PR score is {f1_train}\")\n",
    "print(f\"PR score is {f1_test}\")\n",
    "\n",
    "# accuracy\n",
    "evaluator.setMetricName(\"accuracy\")\n",
    "acc_train = evaluator.evaluate(predict_train)\n",
    "acc_test = evaluator.evaluate(predict_test)\n",
    "print(f\"Accuracy is {acc_train}\")\n",
    "print(f\"Accuracy is {acc_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "894527ec-4ce6-4b32-ad05-e3912098b3ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Testing Accuracy of Raw Elo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75810c15-4288-47bf-919a-27e0fecfa464",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [
        {
         "name": "elo_outcome_df",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "outcome_fide",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "result_int",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create UDF to round expected ELO to binary outcome\n",
    "def predict_outcome_elo(x):\n",
    "    if x <= 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "elo_outcome = udf(predict_outcome_elo, IntegerType())\n",
    "\n",
    "elo_outcome_df = df_eval.withColumn(\"outcome_fide\", elo_outcome(\"expected_score_fide\")).select(\"outcome_fide\", \"result_int\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58297734-2156-4409-afd5-aad4e17cf05b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+------------+----------+\n",
       "outcome_fide|result_int|\n",
       "+------------+----------+\n",
       "           1|         1|\n",
       "           1|         1|\n",
       "           0|         0|\n",
       "           0|         0|\n",
       "           1|         0|\n",
       "           0|         0|\n",
       "           0|         1|\n",
       "           1|         1|\n",
       "           0|         1|\n",
       "           0|         0|\n",
       "           1|         0|\n",
       "           0|         1|\n",
       "           1|         1|\n",
       "           0|         0|\n",
       "           1|         1|\n",
       "           0|         1|\n",
       "           1|         0|\n",
       "           0|         0|\n",
       "           1|         1|\n",
       "           0|         1|\n",
       "+------------+----------+\n",
       "only showing top 20 rows\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+------------+----------+\n|outcome_fide|result_int|\n+------------+----------+\n|           1|         1|\n|           1|         1|\n|           0|         0|\n|           0|         0|\n|           1|         0|\n|           0|         0|\n|           0|         1|\n|           1|         1|\n|           0|         1|\n|           0|         0|\n|           1|         0|\n|           0|         1|\n|           1|         1|\n|           0|         0|\n|           1|         1|\n|           0|         1|\n|           1|         0|\n|           0|         0|\n|           1|         1|\n|           0|         1|\n+------------+----------+\nonly showing top 20 rows\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "elo_outcome_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c511f103-ad1b-4a15-a1bd-1810b13f9819",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">AUC score is 0.7028740872550353\n",
       "PR score is 0.667731860095159\n",
       "F1 score is 0.7027223439205623\n",
       "Accuracy is 0.7030898301616534\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">AUC score is 0.7028740872550353\nPR score is 0.667731860095159\nF1 score is 0.7027223439205623\nAccuracy is 0.7030898301616534\n</div>",
       "datasetInfos": [
        {
         "name": "predictions",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "result_int",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "outcome_fide",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "rawPrediction",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "prediction",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "label",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = elo_outcome_df.select(\"result_int\", \"outcome_fide\").withColumn(\"rawPrediction\", F.col(\"result_int\").cast(DoubleType()))\\\n",
    "                                                                 .withColumn(\"prediction\", F.col(\"result_int\").cast(DoubleType()))\\\n",
    "                                                                 .withColumn(\"label\", F.col(\"outcome_fide\").cast(DoubleType()))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"outcome_fide\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# auc\n",
    "auc_test = evaluator.evaluate(predictions)\n",
    "print(f\"AUC score is {auc_test}\")\n",
    "\n",
    "# pr\n",
    "evaluator.setMetricName(\"areaUnderPR\")\n",
    "pr_test = evaluator.evaluate(predictions)\n",
    "print(f\"PR score is {pr_test}\")\n",
    "\n",
    "# f1\n",
    "evaluator = MulticlassClassificationEvaluator()\\\n",
    "                .setLabelCol(\"label\")\\\n",
    "                .setPredictionCol(\"prediction\")\n",
    "evaluator.setMetricName(\"f1\") \n",
    "f1_test = evaluator.evaluate(predictions)\n",
    "print(f\"F1 score is {f1_test}\")\n",
    "\n",
    "# accuracy\n",
    "evaluator.setMetricName(\"accuracy\")\n",
    "acc_test = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy is {acc_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8941785-eeaa-4713-9b4c-255f62585b1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Testing Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3dd130e-6b1b-459d-9f5d-a1dd17c56b9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|       features|label|\n",
      "+---------------+-----+\n",
      "|[2500.0,2350.0]|    1|\n",
      "|[2331.0,2646.0]|    0|\n",
      "|[2317.0,2287.0]|    0|\n",
      "|[2406.0,2440.0]|    1|\n",
      "|[2544.0,2386.0]|    1|\n",
      "|[2746.0,2778.0]|    1|\n",
      "|[2736.0,2646.0]|    0|\n",
      "|[2665.0,2767.0]|    1|\n",
      "|[2637.0,2785.0]|    0|\n",
      "|[2805.0,2748.0]|    1|\n",
      "|[2878.0,2771.0]|    1|\n",
      "|[2781.0,2868.0]|    1|\n",
      "|[2785.0,2864.0]|    0|\n",
      "|[2909.0,2748.0]|    1|\n",
      "|[2761.0,2908.0]|    0|\n",
      "|[2417.0,2625.0]|    0|\n",
      "|[2453.0,2529.0]|    1|\n",
      "|[2770.0,2514.0]|    1|\n",
      "|[2431.0,2772.0]|    0|\n",
      "|[2926.0,2767.0]|    1|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "va = VectorAssembler(outputCol=\"features\", inputCols=[\"White Elo\", \"Black Elo\"])\n",
    "va_df = va.transform(df_eval).select(\"features\", \"result_int\").withColumnRenamed(\"result_int\", \"label\")\n",
    "\n",
    "va_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3266f3ce-b7df-4184-b7bb-1bac2f6d09f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create train/test split\n",
    "splits = va_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "train = splits[0].cache()\n",
    "test = splits[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b780bd4-5f27-4360-bd15-1d9cb09a3d2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "model = gbt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc484b34-7638-4b77-b810-b5097e674b60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score is 0.7477670143795642\n",
      "PR score is 0.7485644497359304\n",
      "F1 score is 0.683867084367122\n",
      "Accuracy is 0.6845277963831212\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# auc\n",
    "auc_test = evaluator.evaluate(predictions)\n",
    "print(f\"AUC score is {auc_test}\")\n",
    "\n",
    "# pr\n",
    "evaluator.setMetricName(\"areaUnderPR\")\n",
    "pr_test = evaluator.evaluate(predictions)\n",
    "print(f\"PR score is {pr_test}\")\n",
    "\n",
    "# f1\n",
    "evaluator = MulticlassClassificationEvaluator()\\\n",
    "                .setLabelCol(\"label\")\\\n",
    "                .setPredictionCol(\"prediction\")\n",
    "evaluator.setMetricName(\"f1\") \n",
    "f1_test = evaluator.evaluate(predictions)\n",
    "print(f\"F1 score is {f1_test}\")\n",
    "\n",
    "# accuracy\n",
    "evaluator.setMetricName(\"accuracy\")\n",
    "acc_test = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy is {acc_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa91d335-71b6-48f5-b537-62474db51198",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3863892-99b0-4e88-b5f7-f4df46ff6914",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(gbt.maxDepth, [2, 5, 10])\n",
    "              .addGrid(gbt.minInstancesPerNode, [1, 5, 10])\n",
    "              .addGrid(gbt.stepSize, [0.1, 0.01])\n",
    "              .build())\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "cv = CrossValidator(estimator=gbt,\n",
    "                    estimatorParamMaps=param_grid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=5)\n",
    "\n",
    "cv_model = cv.fit(train)\n",
    "\n",
    "best_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2466fb0-bd9a-4bea-ac5a-a0af4fae2fea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = best_model.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# auc\n",
    "auc_test = evaluator.evaluate(predictions)\n",
    "print(f\"AUC score is {auc_test}\")\n",
    "\n",
    "# pr\n",
    "evaluator.setMetricName(\"areaUnderPR\")\n",
    "pr_test = evaluator.evaluate(predictions)\n",
    "print(f\"PR score is {pr_test}\")\n",
    "\n",
    "# f1\n",
    "evaluator = MulticlassClassificationEvaluator()\\\n",
    "                .setLabelCol(\"label\")\\\n",
    "                .setPredictionCol(\"prediction\")\n",
    "evaluator.setMetricName(\"f1\") \n",
    "f1_test = evaluator.evaluate(predictions)\n",
    "print(f\"F1 score is {f1_test}\")\n",
    "\n",
    "# accuracy\n",
    "evaluator.setMetricName(\"accuracy\")\n",
    "acc_test = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy is {acc_test}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "elo_predictor",
   "notebookOrigID": 3451456928319883,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
